{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nSG99FM_zQDu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Brandon\\miniconda3\\envs\\mldl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from typing import Optional, Set, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n",
        "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
        "from transformers.models.vivit.configuration_vivit import VivitConfig\n",
        "from transformers import VivitImageProcessor\n",
        "\n",
        "\n",
        "import av\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYeOfTCDubL2"
      },
      "outputs": [],
      "source": [
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    Decode the video with PyAV decoder.\n",
        "    Args:\n",
        "        container (`av.container.input.InputContainer`): PyAV container.\n",
        "        indices (`List[int]`): List of frame indices to decode.\n",
        "    Returns:\n",
        "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "    '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            frames.append(frame)\n",
        "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    '''\n",
        "    Sample a given number of frame indices from the video.\n",
        "    Args:\n",
        "        clip_len (`int`): Total number of frames to sample.\n",
        "        frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "    Returns:\n",
        "        indices (`List[int]`): List of sampled frame indices\n",
        "    '''\n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    start_idx = end_idx - converted_len\n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBlMT_sqza2z"
      },
      "outputs": [],
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"google/vivit-b-16x2-kinetics400\"\n",
        "_CONFIG_FOR_DOC = \"VivitConfig\"\n",
        "\n",
        "VIVIT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"google/vivit-b-16x2-kinetics400\",\n",
        "    # See all Vivit models at https://huggingface.co/models?filter=vivit\n",
        "]\n",
        "\n",
        "\n",
        "class VivitTubeletEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct Vivit Tubelet embeddings.\n",
        "\n",
        "    This module turns a batch of videos of shape (batch_size, num_frames, num_channels, height, width) into a tensor of\n",
        "    shape (batch_size, seq_len, hidden_size) to be consumed by a Transformer encoder.\n",
        "\n",
        "    The seq_len (the number of patches) equals (number of frames // tubelet_size[0]) * (height // tubelet_size[1]) *\n",
        "    (width // tubelet_size[2]).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_frames = config.num_frames\n",
        "        self.image_size = config.image_size\n",
        "        self.patch_size = config.tubelet_size\n",
        "        self.num_patches = (\n",
        "            (self.image_size // self.patch_size[2])\n",
        "            * (self.image_size // self.patch_size[1])\n",
        "            * (self.num_frames // self.patch_size[0])\n",
        "        )\n",
        "        self.embed_dim = config.hidden_size\n",
        "\n",
        "        self.projection = nn.Conv3d(\n",
        "            config.num_channels, config.hidden_size, kernel_size=config.tubelet_size, stride=config.tubelet_size\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size, num_frames, num_channels, height, width = pixel_values.shape\n",
        "        if height != self.image_size or width != self.image_size:\n",
        "            raise ValueError(\n",
        "                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n",
        "            )\n",
        "\n",
        "        # permute to (batch_size, num_channels, num_frames, height, width)\n",
        "        pixel_values = pixel_values.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        x = self.projection(pixel_values)\n",
        "        # out_batch_size, out_num_channels, out_num_frames, out_height, out_width = x.shape\n",
        "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VivitEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Vivit Embeddings.\n",
        "\n",
        "    Creates embeddings from a video using VivitTubeletEmbeddings, adds CLS token and positional embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "        self.patch_embeddings = VivitTubeletEmbeddings(config)\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(\n",
        "            torch.zeros(1, self.patch_embeddings.num_patches + 1, config.hidden_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        embeddings = self.patch_embeddings(pixel_values)\n",
        "\n",
        "        cls_tokens = self.cls_token.tile([batch_size, 1, 1])\n",
        "\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "\n",
        "        # add positional encoding to each token\n",
        "        embeddings = embeddings + self.position_embeddings\n",
        "\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->Vivit\n",
        "class VivitSelfAttention(nn.Module):\n",
        "    def __init__(self, config: VivitConfig) -> None:\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
        "                f\"heads {config.num_attention_heads}.\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n",
        "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Vivit\n",
        "class VivitSelfOutput(nn.Module):\n",
        "    \"\"\"\n",
        "    The residual connection is defined in VivitLayer instead of here (as is the case with other models), due to the\n",
        "    layernorm applied before each block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: VivitConfig) -> None:\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->Vivit\n",
        "class VivitAttention(nn.Module):\n",
        "    def __init__(self, config: VivitConfig) -> None:\n",
        "        super().__init__()\n",
        "        self.attention = VivitSelfAttention(config)\n",
        "        self.output = VivitSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads: Set[int]) -> None:\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.attention.query = prune_linear_layer(self.attention.query, index)\n",
        "        self.attention.key = prune_linear_layer(self.attention.key, index)\n",
        "        self.attention.value = prune_linear_layer(self.attention.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n",
        "        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
        "        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
        "\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class VivitIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class VivitOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states + input_tensor\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class VivitLayer(nn.Module):\n",
        "    \"\"\"This corresponds to the EncoderBlock class in the scenic/vivit implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = VivitAttention(config)\n",
        "        self.intermediate = VivitIntermediate(config)\n",
        "        self.output = VivitOutput(config)\n",
        "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_attention_outputs = self.attention(\n",
        "            # in Vivit, layernorm is applied before self-attention\n",
        "            self.layernorm_before(hidden_states),\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        # add self attentions if we output attention weights\n",
        "        outputs = self_attention_outputs[1:]\n",
        "\n",
        "        # first residual connection\n",
        "        hidden_states = attention_output + hidden_states\n",
        "\n",
        "        # in Vivit, layernorm is also applied after self-attention\n",
        "        layer_output = self.layernorm_after(hidden_states)\n",
        "        layer_output = self.intermediate(layer_output)\n",
        "\n",
        "        # second residual connection is done here\n",
        "        layer_output = self.output(layer_output, hidden_states)\n",
        "\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class VivitEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([VivitLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                layer_outputs = self._gradient_checkpointing_func(\n",
        "                    layer_module.__call__,\n",
        "                    hidden_states,\n",
        "                    layer_head_mask,\n",
        "                    output_attentions,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class VivitPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class VivitPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = VivitConfig\n",
        "    base_model_prefix = \"vivit\"\n",
        "    main_input_name = \"pixel_values\"\n",
        "    supports_gradient_checkpointing = True\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv3d)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, nn.Parameter):\n",
        "            module.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "\n",
        "\n",
        "VIVIT_START_DOCSTRING = r\"\"\"\n",
        "    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n",
        "    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
        "    behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config ([`VivitConfig`]): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "VIVIT_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n",
        "            Pixel values. Pixel values can be obtained using [`VivitImageProcessor`]. See\n",
        "            [`VivitImageProcessor.preprocess`] for details.\n",
        "\n",
        "        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare ViViT Transformer model outputting raw hidden-states without any specific head on top.\",\n",
        "    VIVIT_START_DOCSTRING,\n",
        ")\n",
        "class VivitModel(VivitPreTrainedModel):\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = VivitEmbeddings(config)\n",
        "        self.encoder = VivitEncoder(config)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.pooler = VivitPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.patch_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model.\n",
        "\n",
        "        Args:\n",
        "            heads_to_prune:\n",
        "                dict of {layer_num: list of heads to prune in this layer}\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(VIVIT_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.FloatTensor], BaseModelOutputWithPooling]:\n",
        "        r\"\"\"\n",
        "        Returns:\n",
        "\n",
        "        Examples:\n",
        "\n",
        "        ```python\n",
        "        >>> import av\n",
        "        >>> import numpy as np\n",
        "\n",
        "        >>> from transformers import VivitImageProcessor, VivitModel\n",
        "        >>> from huggingface_hub import hf_hub_download\n",
        "\n",
        "        >>> np.random.seed(0)\n",
        "\n",
        "\n",
        "        >>> def read_video_pyav(container, indices):\n",
        "        ...     '''\n",
        "        ...     Decode the video with PyAV decoder.\n",
        "        ...     Args:\n",
        "        ...         container (`av.container.input.InputContainer`): PyAV container.\n",
        "        ...         indices (`List[int]`): List of frame indices to decode.\n",
        "        ...     Returns:\n",
        "        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "        ...     '''\n",
        "        ...     frames = []\n",
        "        ...     container.seek(0)\n",
        "        ...     start_index = indices[0]\n",
        "        ...     end_index = indices[-1]\n",
        "        ...     for i, frame in enumerate(container.decode(video=0)):\n",
        "        ...         if i > end_index:\n",
        "        ...             break\n",
        "        ...         if i >= start_index and i in indices:\n",
        "        ...             frames.append(frame)\n",
        "        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "        ...     '''\n",
        "        ...     Sample a given number of frame indices from the video.\n",
        "        ...     Args:\n",
        "        ...         clip_len (`int`): Total number of frames to sample.\n",
        "        ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "        ...     Returns:\n",
        "        ...         indices (`List[int]`): List of sampled frame indices\n",
        "        ...     '''\n",
        "        ...     converted_len = int(clip_len * frame_sample_rate)\n",
        "        ...     end_idx = np.random.randint(converted_len, seg_len)\n",
        "        ...     start_idx = end_idx - converted_len\n",
        "        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "        ...     return indices\n",
        "\n",
        "\n",
        "        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
        "        >>> file_path = hf_hub_download(\n",
        "        ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
        "        ... )\n",
        "        >>> container = av.open(file_path)\n",
        "\n",
        "        >>> # sample 32 frames\n",
        "        >>> indices = sample_frame_indices(clip_len=32, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
        "        >>> video = read_video_pyav(container=container, indices=indices)\n",
        "\n",
        "        >>> image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "        >>> model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "\n",
        "        >>> # prepare video for the model\n",
        "        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n",
        "\n",
        "        >>> # forward pass\n",
        "        >>> outputs = model(**inputs)\n",
        "        >>> last_hidden_states = outputs.last_hidden_state\n",
        "        >>> list(last_hidden_states.shape)\n",
        "        [1, 3137, 768]\n",
        "        ```\"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if pixel_values is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(pixel_values)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        sequence_output = self.layernorm(sequence_output)\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPooling(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"ViViT Transformer model with a video classification head on top (a linear layer on top of the final hidden state of the\n",
        "[CLS] token) e.g. for Kinetics-400.\"\"\",\n",
        "    VIVIT_START_DOCSTRING,\n",
        ")\n",
        "class VivitForVideoClassification(VivitPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.num_labels = config.num_labels\n",
        "        self.vivit = VivitModel(config, add_pooling_layer=False)\n",
        "\n",
        "        # Classifier head\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(VIVIT_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=ImageClassifierOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.FloatTensor], ImageClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Examples:\n",
        "\n",
        "        ```python\n",
        "        >>> import av\n",
        "        >>> import numpy as np\n",
        "        >>> import torch\n",
        "\n",
        "        >>> from transformers import VivitImageProcessor, VivitForVideoClassification\n",
        "        >>> from huggingface_hub import hf_hub_download\n",
        "\n",
        "        >>> np.random.seed(0)\n",
        "\n",
        "\n",
        "        >>> def read_video_pyav(container, indices):\n",
        "        ...     '''\n",
        "        ...     Decode the video with PyAV decoder.\n",
        "        ...     Args:\n",
        "        ...         container (`av.container.input.InputContainer`): PyAV container.\n",
        "        ...         indices (`List[int]`): List of frame indices to decode.\n",
        "        ...     Returns:\n",
        "        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
        "        ...     '''\n",
        "        ...     frames = []\n",
        "        ...     container.seek(0)\n",
        "        ...     start_index = indices[0]\n",
        "        ...     end_index = indices[-1]\n",
        "        ...     for i, frame in enumerate(container.decode(video=0)):\n",
        "        ...         if i > end_index:\n",
        "        ...             break\n",
        "        ...         if i >= start_index and i in indices:\n",
        "        ...             frames.append(frame)\n",
        "        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
        "\n",
        "\n",
        "        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "        ...     '''\n",
        "        ...     Sample a given number of frame indices from the video.\n",
        "        ...     Args:\n",
        "        ...         clip_len (`int`): Total number of frames to sample.\n",
        "        ...         frame_sample_rate (`int`): Sample every n-th frame.\n",
        "        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
        "        ...     Returns:\n",
        "        ...         indices (`List[int]`): List of sampled frame indices\n",
        "        ...     '''\n",
        "        ...     converted_len = int(clip_len * frame_sample_rate)\n",
        "        ...     end_idx = np.random.randint(converted_len, seg_len)\n",
        "        ...     start_idx = end_idx - converted_len\n",
        "        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "        ...     return indices\n",
        "\n",
        "\n",
        "        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n",
        "        >>> file_path = hf_hub_download(\n",
        "        ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
        "        ... )\n",
        "        >>> container = av.open(file_path)\n",
        "\n",
        "        >>> # sample 32 frames\n",
        "        >>> indices = sample_frame_indices(clip_len=32, frame_sample_rate=4, seg_len=container.streams.video[0].frames)\n",
        "        >>> video = read_video_pyav(container=container, indices=indices)\n",
        "\n",
        "        >>> image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "        >>> model = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "\n",
        "        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n",
        "\n",
        "        >>> with torch.no_grad():\n",
        "        ...     outputs = model(**inputs)\n",
        "        ...     logits = outputs.logits\n",
        "\n",
        "        >>> # model predicts one of the 400 Kinetics-400 classes\n",
        "        >>> predicted_label = logits.argmax(-1).item()\n",
        "        >>> print(model.config.id2label[predicted_label])\n",
        "        LABEL_116\n",
        "        ```\"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.vivit(\n",
        "            pixel_values,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.classifier(sequence_output[:, 0, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return ImageClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7bbf8a187d0d4e5db035de7960211ede",
            "92b2981003b34ce4b1249d99969f4918",
            "075e7f61ca8e454fa770fe7fd17896ec",
            "a4b80c4cdb4c426e8ee5f6fd96e1473d",
            "db3a9a3443ef4f998d0eee7fcf0a8304",
            "4fd2e685bc9648658c7a133392b8d178",
            "b4c17b17ca904655802c744e5898f7a6",
            "292208363f0d45e9a1d0001cc8939a93",
            "5cad2b5373c449d8a3b72fffdddd2a68",
            "e10b8c22b6294ec0995930b7aadf3149",
            "c4c3c597a6c14e1aadb9029b2a60425f"
          ]
        },
        "id": "cuPHie5TvYYD",
        "outputId": "7580800e-a391-4a77-b74e-a2fa58d26ea3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bbf8a187d0d4e5db035de7960211ede",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "eating_spaghetti.mp4:   0%|          | 0.00/1.01M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
        ")\n",
        "container = av.open(file_path)\n",
        "\n",
        "# sample 32 frames\n",
        "indices = sample_frame_indices(clip_len=32, frame_sample_rate=4, seg_len=container.streams.video[0].frames)\n",
        "video = read_video_pyav(container=container, indices=indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "c6fb15d117914ecf85d387543155f012",
            "4b9dfc9952614e30ae67028b46cbb710",
            "3ca7df263d3e479fa48b17bb256a53b5",
            "f86f7ce31ee44c1ebe0023e6bab6af5d",
            "5b28ed9a72264ec39dbf2a41feac1fe8",
            "f1d218aadd51494388ba23fcac5936b0",
            "c4ca771afca24cd1b49ad5b8c783a927",
            "ab59ce0a8b24468ab81937ae93e608bd",
            "c13d2932d9334f09afbdd028d23996d4",
            "0a2eade7f7d047f698faa391ca647938",
            "16d68d3ac9534eb694d1f6a0b4667426",
            "7ad641910c2f4fe0aefc8d2adb786e70",
            "a5772afbf9624c63923d63879bb885cd",
            "615dfd1640ac49a18a798a79960b319e",
            "8a64316161214ababcdfb9b951f008ec",
            "6702deeac5784597a13b0035a28c0e44",
            "43f72656842a4b9abdf0019e4bc614b2",
            "914203c5af734bc9a1143b2c885bfdaa",
            "97a02b05eec44dcf859710e5b5587491",
            "ac14785ad8744d668592bd9939784b3d",
            "bf0d7d2883ee4e13bdc85b8eab184da7",
            "4daf71b89c93457997628ed2c714684b",
            "489e3e31b3594b539792e7323d62db23",
            "727147f9915e401481773b6254d3fdd2",
            "0e76e762d8864a649ea12c6acbbc1d38",
            "b3a0bb9c345441f6bf87c463912e1eb3",
            "8a75d1376a2c4117b12435e04262642c",
            "9fb80a27aeb045e0beaeefee26a2c727",
            "631a2bf0598345e59582f3290a4e4acb",
            "f7d4257a2f4b468fba5072b0ed7320d8",
            "57a65f91c9b343e487699e0fea76a665",
            "2e1a7e2c32b94688a05800e46691cbf6",
            "fd557d30caa94eb6836ebd637c1c7ad0"
          ]
        },
        "id": "PkNKXV2QvmZv",
        "outputId": "ac83a4f6-9294-4f2e-ba77-d3fdbd95fb63"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6fb15d117914ecf85d387543155f012",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ad641910c2f4fe0aefc8d2adb786e70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/18.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "489e3e31b3594b539792e7323d62db23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/356M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
        "model = VivitForVideoClassification.from_pretrained(\"google/vivit-b-16x2-kinetics400\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAMPOqtQvvgw",
        "outputId": "654a8635-7f7f-401d-aaca-e0692c49027b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  return torch.tensor(value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LABEL_116\n"
          ]
        }
      ],
      "source": [
        "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# model predicts one of the 400 Kinetics-400 classes\n",
        "predicted_label = logits.argmax(-1).item()\n",
        "print(model.config.id2label[predicted_label])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "075e7f61ca8e454fa770fe7fd17896ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_292208363f0d45e9a1d0001cc8939a93",
            "max": 1013655,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5cad2b5373c449d8a3b72fffdddd2a68",
            "value": 1013655
          }
        },
        "0a2eade7f7d047f698faa391ca647938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e76e762d8864a649ea12c6acbbc1d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7d4257a2f4b468fba5072b0ed7320d8",
            "max": 355881581,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57a65f91c9b343e487699e0fea76a665",
            "value": 355881581
          }
        },
        "16d68d3ac9534eb694d1f6a0b4667426": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "292208363f0d45e9a1d0001cc8939a93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e1a7e2c32b94688a05800e46691cbf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ca7df263d3e479fa48b17bb256a53b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab59ce0a8b24468ab81937ae93e608bd",
            "max": 401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c13d2932d9334f09afbdd028d23996d4",
            "value": 401
          }
        },
        "43f72656842a4b9abdf0019e4bc614b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489e3e31b3594b539792e7323d62db23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_727147f9915e401481773b6254d3fdd2",
              "IPY_MODEL_0e76e762d8864a649ea12c6acbbc1d38",
              "IPY_MODEL_b3a0bb9c345441f6bf87c463912e1eb3"
            ],
            "layout": "IPY_MODEL_8a75d1376a2c4117b12435e04262642c"
          }
        },
        "4b9dfc9952614e30ae67028b46cbb710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1d218aadd51494388ba23fcac5936b0",
            "placeholder": "​",
            "style": "IPY_MODEL_c4ca771afca24cd1b49ad5b8c783a927",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "4daf71b89c93457997628ed2c714684b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fd2e685bc9648658c7a133392b8d178": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a65f91c9b343e487699e0fea76a665": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b28ed9a72264ec39dbf2a41feac1fe8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cad2b5373c449d8a3b72fffdddd2a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "615dfd1640ac49a18a798a79960b319e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97a02b05eec44dcf859710e5b5587491",
            "max": 18554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac14785ad8744d668592bd9939784b3d",
            "value": 18554
          }
        },
        "631a2bf0598345e59582f3290a4e4acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6702deeac5784597a13b0035a28c0e44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "727147f9915e401481773b6254d3fdd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fb80a27aeb045e0beaeefee26a2c727",
            "placeholder": "​",
            "style": "IPY_MODEL_631a2bf0598345e59582f3290a4e4acb",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "7ad641910c2f4fe0aefc8d2adb786e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5772afbf9624c63923d63879bb885cd",
              "IPY_MODEL_615dfd1640ac49a18a798a79960b319e",
              "IPY_MODEL_8a64316161214ababcdfb9b951f008ec"
            ],
            "layout": "IPY_MODEL_6702deeac5784597a13b0035a28c0e44"
          }
        },
        "7bbf8a187d0d4e5db035de7960211ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92b2981003b34ce4b1249d99969f4918",
              "IPY_MODEL_075e7f61ca8e454fa770fe7fd17896ec",
              "IPY_MODEL_a4b80c4cdb4c426e8ee5f6fd96e1473d"
            ],
            "layout": "IPY_MODEL_db3a9a3443ef4f998d0eee7fcf0a8304"
          }
        },
        "8a64316161214ababcdfb9b951f008ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf0d7d2883ee4e13bdc85b8eab184da7",
            "placeholder": "​",
            "style": "IPY_MODEL_4daf71b89c93457997628ed2c714684b",
            "value": " 18.6k/18.6k [00:00&lt;00:00, 802kB/s]"
          }
        },
        "8a75d1376a2c4117b12435e04262642c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "914203c5af734bc9a1143b2c885bfdaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92b2981003b34ce4b1249d99969f4918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fd2e685bc9648658c7a133392b8d178",
            "placeholder": "​",
            "style": "IPY_MODEL_b4c17b17ca904655802c744e5898f7a6",
            "value": "eating_spaghetti.mp4: 100%"
          }
        },
        "97a02b05eec44dcf859710e5b5587491": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb80a27aeb045e0beaeefee26a2c727": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b80c4cdb4c426e8ee5f6fd96e1473d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e10b8c22b6294ec0995930b7aadf3149",
            "placeholder": "​",
            "style": "IPY_MODEL_c4c3c597a6c14e1aadb9029b2a60425f",
            "value": " 1.01M/1.01M [00:00&lt;00:00, 7.07MB/s]"
          }
        },
        "a5772afbf9624c63923d63879bb885cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f72656842a4b9abdf0019e4bc614b2",
            "placeholder": "​",
            "style": "IPY_MODEL_914203c5af734bc9a1143b2c885bfdaa",
            "value": "config.json: 100%"
          }
        },
        "ab59ce0a8b24468ab81937ae93e608bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac14785ad8744d668592bd9939784b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3a0bb9c345441f6bf87c463912e1eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e1a7e2c32b94688a05800e46691cbf6",
            "placeholder": "​",
            "style": "IPY_MODEL_fd557d30caa94eb6836ebd637c1c7ad0",
            "value": " 356M/356M [00:06&lt;00:00, 55.3MB/s]"
          }
        },
        "b4c17b17ca904655802c744e5898f7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf0d7d2883ee4e13bdc85b8eab184da7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13d2932d9334f09afbdd028d23996d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4c3c597a6c14e1aadb9029b2a60425f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4ca771afca24cd1b49ad5b8c783a927": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6fb15d117914ecf85d387543155f012": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b9dfc9952614e30ae67028b46cbb710",
              "IPY_MODEL_3ca7df263d3e479fa48b17bb256a53b5",
              "IPY_MODEL_f86f7ce31ee44c1ebe0023e6bab6af5d"
            ],
            "layout": "IPY_MODEL_5b28ed9a72264ec39dbf2a41feac1fe8"
          }
        },
        "db3a9a3443ef4f998d0eee7fcf0a8304": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e10b8c22b6294ec0995930b7aadf3149": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1d218aadd51494388ba23fcac5936b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7d4257a2f4b468fba5072b0ed7320d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f86f7ce31ee44c1ebe0023e6bab6af5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a2eade7f7d047f698faa391ca647938",
            "placeholder": "​",
            "style": "IPY_MODEL_16d68d3ac9534eb694d1f6a0b4667426",
            "value": " 401/401 [00:00&lt;00:00, 24.0kB/s]"
          }
        },
        "fd557d30caa94eb6836ebd637c1c7ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
